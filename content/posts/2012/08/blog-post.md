Title: Виды поисковых роботов
Author: d.rey
Date: 2012-08-01 11:59:00
Slug: blog-post
Tags: engine,search,robots

[Поисковый робот](http://ru.wikipedia.org/wiki/%D0%9F%D0%BE%D0%B8%D1%81%D0%BA%D0%BE%D0%B2%D1%8B%D0%B9_%D1%80%D0%BE%D0%B1%D0%BE%D1%82) ('веб-паук', краулер) — программа, являющаяся составной частью поисковой системы и предназначена для перебора страниц Интернета с целью занесения информации о них в базу данных поисковика. По принципу действия паук напоминает обычный браузер. Он анализирует содержимое страницы, сохраняет его в некотором специальном виде на сервере поисковой машины, которой принадлежит, и отправляет запросы по ссылкам на следующие страницы. (c) Wikipedia

Существует несколько различных сценариев использования поисковых роботов для сбора данных. Ниже будут рассмотрены несколько их примеров. Обзор представлен на основании научной работы [Design and Implementation of a High-Performance Distributed Web Crawler. Vladislav Shkapenyuk, Torsten Suel](http://cis.poly.edu/suel/papers/crawl.pdf)

**Поиск в ширину**

Для того чтобы построить сложную поисковую систему или большой репозиторий, такой как [Internet Archive](http://archive.org/), поисковые роботы используют на начальном этапе небольшой набор ссылок. После исследования страниц из этого набора, выбраются ссылки и уже по ним осуществляется дальнейший [поиск (поиск в ширину)](http://ru.wikipedia.org/wiki/%D0%9F%D0%BE%D0%B8%D1%81%D0%BA_%D0%B2_%D1%88%D0%B8%D1%80%D0%B8%D0%BD%D1%83). В реальной жизни веб страницы редко выбираются строго согласно этого подхода. Чаще всего применяются различные политики, например исследования только внутри определенного веб сайта или сперва осуществляется анализ страниц с более высоким приоритетом, выбранных на основании неких критериев.

**Повторный обход страниц с целью поиска обновленной информации**

После того как страницы были прогружены и проанализированы, возможно потребуется повторная проверка содержимого этих страниц с целью определения обновленной  информации. Они могут быть снова пройдены с помощью поиска в ширину или опросом всех существующих URL из коллекции. Могут быть также применены различные эвристические подходы с целью выборки наиболее важных страниц, веб сайтов или доменов, для проверки их в первую очередь.

**Направленный поиск**

Наиболее специализированные поисковые системы могут быть сфокусированы на анализ и поиск только определенного типа страниц: страниц определенной тематики, с определенным языком, содержащими изображения, файлы или как пример, научные статьи. В дополнении к эвристическим методам применяются методы основанные на выборке по определенному шаблону URL, применяются технологии машинного обучения. Оснавная задача навправленных поисковых систем - это нахождение наибольшого числа страниц по заданным критерием без необходимости перебора всех страниц подряд. Направленный поиск позволяют создавать большие коллекции документов, которые наиболее актуальны по дате, чем коллекции, созданные поисковыми системами, ориентированные на более широкий обзор ресурсов/страниц.

**Случайный проход**

Некоторые техники позволяют использовать технологии случайного прохода по графу веб страниц для получения информации о странице, ее размера с целью проверки качества работы поисковой системы.

**Поиск 'скрытых' веб страниц**

Большие объемы информации в веб находятся в базах данных и доступны только по соотвествующим запросам через веб формы. Специальные техники позволяют выполнять автоматический доступ к таким данным ('Hidden Web', 'Deep Web', or 'Federated Facts and Figures'). В данном случае поисковые системы становятся фронтендом для получения данных с подобных ресурсов.

