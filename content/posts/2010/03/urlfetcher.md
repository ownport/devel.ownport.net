Title: UrlFetcher или как собирать данные в Интернет
Author: d.rey
Date: 2010-03-10 10:37:00
Slug: urlfetcher
Tags: fetch,data munging,python,internet

Мне кажется, что каждый человек, который уже достаточно давно знаком с Интернетом сталкивался с ситуацией, когда нужно периодически просматривать информацию на какой-либо странице. Хорошим примером может быть просмотр странички с прогнозом погоды за неделю до отпуска.

Скрипт [UrlFetcher ](http://code.google.com/p/sources-ownport/source/browse/net/urlfetcher.py)о котором я хочу рассказать представляет собой небольшое консольное приложение, которое загружает HTML страницы (без изображений, таблиц стилей, внешних JavaScript-ов и т.д. и т.п.) и заносит их содержимое в базу данных. В случае, если контрольная сумма страницы при загрузке не соответствует значению, сохраненному ранее в базе данных, - данные обновляются (таблица url_data), а старые данные  переносятся в журнал (таблица journal). В качестве базы данных используется sqlite3 (http://www.sqlite.org/), поддержка которой включена в python начиная с версии 2.5. Установка дополнительных модулей не требуется, достаточно лишь наличие установленного python версии 2.5 и выше.

Интерфейс скрипта достаточно прост:

    :::
    net>python urlfetcher.py -h
    Usage: urlfetcher.py [options]
    
    Options:
      -h, --help            show this help message and exit
      -d DATABASE, --database=DATABASE
                            UrlFetcher database, by default: urlfetcher.sqlite
      -a ADD_URL, --add_url=ADD_URL
                            add url for fecthing
      -l, --url_list        printout url list from database
      -u, --update_urls_data
                            update URLs data

База данных в которой будут сохранены данные задается ключом -d <database>. Если база данных не задана, данные будут сохранены в файле urlfetcher.sqlite

    :::
    python urlfetcher.py -d news.sqlite

Для добавления URL в базу данных необходимо воспользоваться ключом -a <url>. 

    :::
    python urlfetcher.py -d news.sqlite -a http://news.google.com/?output=rss

Для отображения всего списка URL-ов в базе используется ключ -l: 

    :::
    python urlfetcher.py -d news.sqlite -l

Обновление данных выполняется с помощью ключа -u:

    :::
    python urlfetcher.py -d news.sqlite -u

Конечно же нельзя не упомянуть, что у данного решения есть недостаток: данные полученные из интернета сравниваются с данными из базы на основании результата хеш-функции, на базе md5. В итоге, достаточно незначительного изменения содержимого страницы и скрипт будет понимать, что произошло изменение и данные в базе данных необходимо изменить: старые данные перенести в журнал (таблица journal), а текущие данные обновить (таблица url_data). Примером может служить строка:

    :::
    <!-- Generated on Tue, 09 Mar 2010 22:55:33 -0700 -->

При каждом обращении к такой странице строка с информацией о дате и времени генерации будет меняться, что приведет к изменению контрольной суммы страницы, с учетом того, что содержимое страницы осталось без изменений.

Для решения данного вопроса, сравнение по контрольной сумме недостаточно и необходимо применять методы сравнения по содержимому. Этот вопрос достаточно обширен, поэтому я хочу выделить его в отдельном посте или постах.

P.S. Данный скрипт не является законченным приложением. Основная его задача - это оценить возможности сбора и анализа данных, определиться с перечнем вопросов, которые могут возникнуть при решении подобного рода задач.

P.SS. Как всегда замечания/комментарии/исправления/дополнения только приветствуются.

